{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: Importing libraries\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b83c5f0c63c4602"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from decimal import Decimal"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:31.895714Z",
     "start_time": "2024-03-31T17:41:31.891209Z"
    }
   },
   "id": "42ac8181c9a20a4e",
   "execution_count": 231
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: We will have total 9 files so we will process them sequentially. First lets define column names and data type."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f06ec8fde35a14fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "directory = '../datasets/original_dataset'\n",
    "cleaned_directory = '../datasets/cleaned_data'\n",
    "\n",
    "column_data_types = {\n",
    "    'olist_customers_dataset.csv' : {\"customer_id\": np.str_,\n",
    "                                     \"customer_unique_id\": np.str_,\n",
    "                                     \"customer_zip_code_prefix\": np.int64,\n",
    "                                     \"customer_city\": np.str_,\n",
    "                                     \"customer_state\": np.str_},\n",
    "    'olist_geolocation_dataset.csv' : {\"geolocation_zip_code_prefix\": np.int64,\n",
    "                                       \"geolocation_lat\": Decimal, # for precision\n",
    "                                       \"geolocation_lng\": Decimal,\n",
    "                                       \"geolocation_city\": np.str_,\n",
    "                                       \"geolocation_state\": np.str_},\n",
    "    'olist_order_items_dataset.csv' : {\"order_id\": np.str_,\n",
    "                                       \"order_item_id\": np.int64,\n",
    "                                       \"product_id\": np.str_,\n",
    "                                       \"seller_id\": np.str_,\n",
    "                                       \"shipping_limit_date\": np.datetime64,\n",
    "                                       \"price\": np.float128,\n",
    "                                       \"freight_value\": np.float128},\n",
    "    'olist_order_payments_dataset.csv': {\"order_id\": np.str_,\n",
    "                                         \"payment_sequential\": np.int64,\n",
    "                                         \"payment_type\": np.str_,\n",
    "                                         \"payment_installments\": np.int64,\n",
    "                                         \"payment_value\": np.float128},\n",
    "    'olist_order_reviews_dataset.csv': {\"review_id\": np.str_,\n",
    "                                        \"order_id\": np.str_,\n",
    "                                        \"review_score\": np.float128,\n",
    "                                        \"review_comment_title\": np.str_,\n",
    "                                        \"review_comment_message\": np.str_,\n",
    "                                        \"review_creation_date\": np.datetime64,\n",
    "                                        \"review_answer_timestamp\": np.datetime64},\n",
    "    'olist_orders_dataset.csv' : {\"order_id\": np.str_,\n",
    "                                  \"customer_id\": np.str_,\n",
    "                                  \"order_status\": np.str_,\n",
    "                                  \"order_purchase_timestamp\": np.datetime64,\n",
    "                                  \"order_approved_at\": np.datetime64,\n",
    "                                  \"order_delivered_carrier_date\": np.datetime64,\n",
    "                                  \"order_delivered_customer_date\": np.datetime64,\n",
    "                                  \"order_estimated_delivery_date\": np.datetime64},\n",
    "    'olist_products_dataset.csv': {\"product_id\": np.str_,\n",
    "                                   \"product_category_name\": np.str_,\n",
    "                                   \"product_name_lenght\": np.int64,\n",
    "                                   \"product_description_lenght\": np.int64,\n",
    "                                   \"product_photos_qty\": np.int64,\n",
    "                                   \"product_weight_g\": np.float128,\n",
    "                                   \"product_length_cm\": np.float128,\n",
    "                                   \"product_height_cm\": np.float128,\n",
    "                                   \"product_width_cm\": np.float128},\n",
    "    'olist_sellers_dataset.csv' : {\"seller_id\": np.str_,\n",
    "                                   \"seller_zip_code_prefix\": np.int64,\n",
    "                                   \"seller_city\": np.str_,\n",
    "                                   \"seller_state\": np.str_},\n",
    "    'product_category_name_translation.csv': {'product_category_name': np.str_,\n",
    "                                              'product_category_name_english': np.str_}\n",
    "}\n",
    "primary_key_intro = {\n",
    "    'olist_customers_dataset.csv' : [\"customer_id\",\"customer_unique_id\"],\n",
    "    'olist_geolocation_dataset.csv' : [\"geolocation_lat\", \"geolocation_lng\"],\n",
    "    'olist_order_items_dataset.csv' : [\"order_id\", \"order_item_id\"],\n",
    "    'olist_order_payments_dataset.csv': [\"order_id\", \"payment_sequential\"],\n",
    "    'olist_orders_dataset.csv' : \"order_id\",\n",
    "    'olist_products_dataset.csv': \"product_id\",\n",
    "    'olist_sellers_dataset.csv' : \"seller_id\",\n",
    "    'product_category_name_translation.csv': 'product_category_name'\n",
    "}\n",
    "\n",
    "foreign_key_check = {\n",
    "    'olist_customers_dataset.csv' : [['olist_orders_dataset.csv'], 'customer_id'],\n",
    "    'olist_geolocation_dataset.csv' : [['olist_customers_dataset.csv', 'olist_sellers_dataset.csv'], 'zip_code_prefix'],\n",
    "    'olist_order_reviews_dataset.csv': [['olist_orders_dataset.csv'], 'order_id'],\n",
    "    'olist_products_dataset.csv': [['olist_order_items_dataset.csv'], 'product_id'],\n",
    "    'olist_sellers_dataset.csv' : [['olist_order_items_dataset.csv'], 'seller_id'],\n",
    "    'product_category_name_translation.csv': [['olist_products_dataset.csv'],'product_category_name'],\n",
    "}\n",
    "\n",
    "pesos_usd = {\n",
    "    'olist_order_items_dataset.csv': ['price','freight_value'],\n",
    "    'olist_order_payments_dataset.csv': ['payment_value'],\n",
    "}\n",
    "\n",
    "category_labeling = {\n",
    "    'product_category_name_translation.csv': 'olist_products_dataset.csv'\n",
    "}\n",
    "\n",
    "wrong_column_name = {\n",
    "    'olist_products_dataset.csv': {\"product_name_lenght\",\n",
    "                                   \"product_description_lenght\"},\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:31.946641Z",
     "start_time": "2024-03-31T17:41:31.935221Z"
    }
   },
   "id": "384a3f7cc91dbc2f",
   "execution_count": 232
  },
  {
   "cell_type": "markdown",
   "source": [
    "some functions which will process the data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b28a098f485e3206"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def pesos_to_usd(pesos):\n",
    "    return pesos * 0.060"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.259337Z",
     "start_time": "2024-03-31T17:41:32.253785Z"
    }
   },
   "id": "a7de4d2a5a118c16",
   "execution_count": 233
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: Next, we will collect all file paths from the directory and keep it inside a list.\n",
    "\n",
    "After that, we will process each file at a time, for the assigned data types we will process null values, wrong/duplicate values and transform to input into database.\n",
    "\n",
    "Finally, we will generate csv file with the cleaned data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47e553a0bcc8be68"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def handle_null_values(df, file):\n",
    "    \n",
    "    column_and_types = column_data_types[file]\n",
    "    \n",
    "    if column_data_types is not None:\n",
    "        for col, col_type in column_and_types.items():\n",
    "            if col_type == np.str_:\n",
    "                df[col] = df[col].fillna('')\n",
    "            elif col_type == np.int64 or col_type == np.float128:\n",
    "                df[col] = df[col].fillna(0)\n",
    "            elif col_type == np.datetime64:\n",
    "                df[col] = df[col].fillna(pd.Timestamp('0001-01-01 00:00:00'))\n",
    "                \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.321026Z",
     "start_time": "2024-03-31T17:41:32.313948Z"
    }
   },
   "id": "9af182f14f99040d",
   "execution_count": 234
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def ensure_foreign_key(df, directory, file):\n",
    "    \n",
    "    foreign_key_table = foreign_key_check.get(file)\n",
    "    \n",
    "    if foreign_key_table:\n",
    "        related_files, key_column = foreign_key_table\n",
    "        \n",
    "        for related_file in related_files:\n",
    "            related_file_path = os.path.join(directory, related_file)\n",
    "            related_df = pd.read_csv(related_file_path)\n",
    "            \n",
    "            related_column = next((col for col in related_df.columns if key_column in col), None)\n",
    "            df_column = next((col for col in df.columns if key_column in col), None)\n",
    "            \n",
    "            missing_ids = related_df[~related_df[related_column].isin(df[df_column])][related_column]\n",
    "            missing_df = pd.DataFrame({related_column: missing_ids}).dropna().drop_duplicates()\n",
    "           \n",
    "            df = pd.concat([df, missing_df], ignore_index=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.345748Z",
     "start_time": "2024-03-31T17:41:32.338944Z"
    }
   },
   "id": "b8f375ac75d8f333",
   "execution_count": 235
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def remove_duplicates(df, file):\n",
    "    primary_key = primary_key_intro.get(file)\n",
    "    \n",
    "    if primary_key:\n",
    "        if isinstance(primary_key, str):\n",
    "            primary_key = [primary_key]\n",
    "        df.drop_duplicates(subset=primary_key, inplace=True)\n",
    "        \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.374729Z",
     "start_time": "2024-03-31T17:41:32.369386Z"
    }
   },
   "id": "5824698e41f8c2c8",
   "execution_count": 236
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def convert_currency(df):\n",
    "    #currency_conversion = pesos_usd.get(file)\n",
    "    \n",
    "    if file in pesos_usd:\n",
    "        for col in pesos_usd[file]:\n",
    "            df[col] = df[col].apply(pesos_to_usd)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.396573Z",
     "start_time": "2024-03-31T17:41:32.390195Z"
    }
   },
   "id": "89a56746464a16a8",
   "execution_count": 237
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def encode_categorical(df, file, cleaned_directory):\n",
    "    category_label = category_labeling.get(file)\n",
    "    \n",
    "    if category_label:\n",
    "        label_encoder = LabelEncoder()\n",
    "        df = df._append({'product_category_name': 'nocategory','product_category_name_english': 'nocategory'}, ignore_index=True)\n",
    "        \n",
    "        df['product_category_id'] = label_encoder.fit_transform(df['product_category_name'])\n",
    "\n",
    "        product_file = os.path.join(cleaned_directory, category_label)\n",
    "        product_df = pd.read_csv(product_file)\n",
    "        \n",
    "        category_mapping = dict(zip(df['product_category_name'], df['product_category_id']))\n",
    "        product_df['product_category_id'] = product_df['product_category_name'].fillna('nocategory').map(category_mapping).astype(np.int64)\n",
    "        product_df.drop(columns=['product_category_name'], inplace=True)\n",
    "        \n",
    "        product_df.to_csv(product_file, index=False)\n",
    "        \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.423827Z",
     "start_time": "2024-03-31T17:41:32.415548Z"
    }
   },
   "id": "be78729d98db8b65",
   "execution_count": 238
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def validating_data(df, file):\n",
    "    \n",
    "    #Fix translation Using google API\n",
    "    \n",
    "    \n",
    "    #for ensuring all int, float values > 0\n",
    "    for col in df.select_dtypes(include=[np.int64, np.float128]).columns:\n",
    "        if (df[col] < 0).any():\n",
    "            df[col] = df[col].apply(lambda x: abs(x))\n",
    "            \n",
    "    #Filling all empty string with unknown\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].fillna('Unknown')\n",
    "        \n",
    "    #correct wrong column names(manually checked, and changing)\n",
    "    column_names = wrong_column_name.get(file)\n",
    "    if column_names:\n",
    "        for column in column_names:\n",
    "            correct_column = column.replace('lenght', 'length')\n",
    "            df.rename(columns={column: correct_column}, inplace=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:32.444744Z",
     "start_time": "2024-03-31T17:41:32.438154Z"
    }
   },
   "id": "bc55094d43c34ccd",
   "execution_count": 239
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for file in os.listdir(directory):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "         #Foreign key ensuring cross-examination\n",
    "        df = ensure_foreign_key(df, directory, file)\n",
    "        \n",
    "        #Handling null values\n",
    "        df = handle_null_values(df, file)\n",
    "        \n",
    "        #Removing duplicates based on primary key(tokenization)\n",
    "        df = remove_duplicates(df, file)\n",
    "        \n",
    "        #Standardizing & normalizing data (basic currency conversion)\n",
    "        df = convert_currency(df)\n",
    "        \n",
    "        \n",
    "        #Encoding categorical variables\n",
    "        df = encode_categorical(df, file, cleaned_directory)\n",
    "        \n",
    "        #Missing value fill, validating data, translation, text correction\n",
    "        df = validating_data(df, file)\n",
    "            \n",
    "        df.to_csv(os.path.join(cleaned_directory,file), index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:41:47.138449Z",
     "start_time": "2024-03-31T17:41:32.474490Z"
    }
   },
   "id": "748b6f97c7e298ef",
   "execution_count": 240
  },
  {
   "cell_type": "markdown",
   "source": [
    "Handling outliers\n",
    "Handling missing or incorrect or cleanup values more specifically\n",
    "\n",
    "\n",
    "\n",
    "Handling outliers:\n",
    "\n",
    "Use descriptive statistics and visualization techniques to identify outliers in your data.\n",
    "Decide on an appropriate method to handle outliers, such as trimming, winsorizing, or transformation.\n",
    "Implement the chosen method using NumPy or Pandas.\n",
    "Standardizing or normalizing data:\n",
    "\n",
    "Choose a scaling method based on the distribution of your data and the requirements of your analysis.\n",
    "Use Scikit-learn's preprocessing module to apply min-max scaling (MinMaxScaler) or z-score normalization (StandardScaler) to your numerical features.\n",
    "Encoding categorical variables:\n",
    "\n",
    "Convert categorical variables into numerical representations using one-hot encoding or label encoding.\n",
    "Pandas provides convenient functions like get_dummies() for one-hot encoding and factorize() for label encoding.\n",
    "Handling missing or incorrect values:\n",
    "\n",
    "Identify missing or incorrect values in your dataset using descriptive statistics or visualization.\n",
    "Decide on an appropriate strategy for handling missing values, such as imputation, removal, or correction.\n",
    "Implement the chosen strategy using Pandas or Scikit-learn's SimpleImputer for imputation or dropna() for removal."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5dd0faa9f92caa4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
